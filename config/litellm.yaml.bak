# Single OpenAI-style endpoint at http://localhost:4000/v1
litellm_settings:
  telemetry: false
  drop_params: false
  # (Optional) add: "num_retries: 2", "timeout: 120"

# Define friendly model names your UIs will use
model_list:
  - model_name: gemma3-4b-q4
      litellm_params:
      model: ollama/gemma3:4b-it-qat
      api_base: http://ollama:11434

  - model_name: llama3.1-8b-q4
    litellm_params:
      model: ollama/llama3.1:8b-instruct-q4_K_M
      api_base: http://ollama:11434

  - model_name: qwen2.5-7b-q4
    litellm_params:
      model: ollama/qwen2.5:7b-instruct-q4_K_M
      api_base: http://ollama:11434

  # Route to vLLM (full OpenAI-compatible server; great throughput)
  - model_name: gemma3-4b-it
    litellm_params:
      model: openai/gemma-3-4b-it
      api_base: http://vllm:8000/v1
      api_key: dev-local-key

# (Optional) Basic key to require from clients; comment out to disable
general_settings:
    master_key: dev-local-key
    allow_requests_on_db_unavailable: true
